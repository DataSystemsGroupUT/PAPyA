{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a library to find the best performing configuration from a set of dimensions (i.e. schemas, partition, storage) which can be specified inside the <b>settings.yaml</b> file in the resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install PAPyA==0.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration file and log files location for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"settings.yaml\" # config file location\n",
    "logs = \"log\" # logs file location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration file <br>\n",
    "the configuration is a yaml file which has two main parts, the dimensions and the number of query experiments. You can add more dimensions here or change these existing dimensions to anything you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "dimensions:\n",
    "    schemas: [\"st\", \"vt\", \"pt\", \"extvt\", \"wpt\"]\n",
    "    partition: [\"horizontal\", \"predicate\", \"subject\"]\n",
    "    storage: [\"csv\", \"avro\", \"parquet\", \"orc\"]\n",
    "\n",
    "query: 11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log file structure <br>\n",
    "the structure of the log files must follow the order of dimensions in the configuration file (i.e. {schemas}.{partition}.{storage}.txt) and the subfolders should be the ranking sets of the experiments (i.e. dataset sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "UI Module\n",
    "└───log\n",
    "    │\n",
    "    |───100M\n",
    "    |    │   st.horizontal.csv.txt\n",
    "    |    │   st.horizontal.avro.txt\n",
    "    |    │   ...\n",
    "    │\n",
    "    └───250M\n",
    "        |   st.horizontal.csv.txt\n",
    "        │   st.horizontal.avro.txt\n",
    "        │   ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Dimensional Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SDRank</b> is a class module from PAPyA library to calculate ranking score _R_ for each dimension independently that operates over a log-based structure that user specified on the configuration file.<br> \n",
    "The value of _R_ represents the performance of a particular configuration (higher value means better performing configuration). We used Ranking Function _R_ below to calculate the rank scores:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$R =\\sum \\limits _{r=1} ^{d} \\frac{O_{dim} * (d-r)}{|Q| * (d-1)}, 0<R<=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$d$         : total number of parameters (options) in a particular dimension<br>\n",
    "$O_{dim}$   : number of occurences of the dimension placed at rank $r$ (Rank 1, Rank 2, Rank 3, ...)<br>\n",
    "$|Q|$       : total number of queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class takes single dimension and dataset sizes as parameters that user specified inside their log files\n",
    "from PAPyA.Rank import SDRank\n",
    "\n",
    "schemaSDRank = SDRank(config, logs, '100M', 'schemas')\n",
    "partitionSDRank = SDRank(config, logs, '100M', 'partition')\n",
    "storageSDRank = SDRank(config, logs, '100M', 'storage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>calculateRank</b> is the function that automates calculating the rank scores of a single dimension using the Ranking Function above.<br>\n",
    "The output of this method is a table of configurations which is sorted based on the best performing configuration according to their Ranking Score along with number of occurences of the dimension being placed at the rank _r_ (1st, 2nd, 3rd, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the top 5 rank scores of configurations in schema dimension sorted from best to worst\n",
    "schemaSDRank.calculateRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the top 5 rank scores of configurations in storage dimension sorted from best to worst\n",
    "storageSDRank.calculateRank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the top 5 rank scores of configurations in partition dimension sorted from best to worst\n",
    "partitionSDRank.calculateRank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ranking over one dimension is insufficient when it counts multiple dimensions. The presence of trade-offs reduces the accuracy of single dimension ranking functions which could be seen in the radar plot below. <br>\n",
    "<b>plotRadar</b> is a method to show the presence of trade-offs by using the single dimension ranking criterion that reduces the accuracy of the other dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_schemaSDRank.plotRadar()_ shows a figure of the top configuration (vt.predicate.avro) of ranking by schema is optimized towards schema dimension only, ignoring the other two dimension.<br>\n",
    "The same can be said for ranking with the other dimensions (storage and partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaSDRank = SDRank(config, logs, '100M', 'schemas')\n",
    "schemaSDRank.plotRadar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storageSDRank = SDRank(config, logs, '100M', 'storage')\n",
    "storageSDRank.plotRadar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionSDRank = SDRank(config, logs, '100M', 'partition')\n",
    "partitionSDRank.plotRadar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to radar plot, PAPyA also provides visualization that shows the performance of a single dimension parameters that user can choose in terms of their rank scores<br>\n",
    "This <b>plot</b> method takes a single argument which is the view projection option that user can specify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of schema dimension plots\n",
    "schemaSDRank = SDRank(config, logs, '100M', 'schemas')\n",
    "\n",
    "schemaSDRank.plot('parquet')\n",
    "schemaSDRank.plot('horizontal')\n",
    "schemaSDRank.plot('predicate')\n",
    "# example of storage dimension plots\n",
    "storageSDRank = SDRank(config, logs, '100M', 'storage')\n",
    "\n",
    "storageSDRank.plot('st')\n",
    "storageSDRank.plot('vp')\n",
    "storageSDRank.plot('horizontal')\n",
    "storageSDRank.plot('subject')\n",
    "# example of partition dimension plots\n",
    "partitionSDRank = SDRank(config, logs, '100M', 'partition')\n",
    "\n",
    "partitionSDRank.plot('wpt')\n",
    "partitionSDRank.plot('extvp')\n",
    "partitionSDRank.plot('csv')\n",
    "partitionSDRank.plot('avro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Dimensional Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the presence of the trade-offs introduced in the single dimensional ranking function, we propose an optimization technique that aims to find the non-dominated solutions or the configuration combinations by optimizing all dimensions at the same time which utilize the NSGA2 Algorithm.<br>\n",
    "In this experiment, we provide two ways to use the NSGA2 Algorithm:\n",
    "- The first method is _paretoAgg_ which operates on the single dimensional ranking criteria. This method aims to maximize performance of the three ranks altogether\n",
    "- The second method is _paretoQ_ which apply the algorithm considering the rank sets obtained by sorting each query results individually. This method aims at minimizing query runtimes of the ranked dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>MDRank</b> is the class module from PAPyA library to perform the multi dimensional optimization of NSGA2.<br>\n",
    "This class takes the ranking sets of the experiments (i.e. dataset sizes) as parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of MDRank class with 100M dataset size as ranking set of the experiment\n",
    "from PAPyA.Rank import MDRank\n",
    "\n",
    "multiDimensionRank = MDRank(config, logs, '100M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of these methods are the table of the best configurations solution of all dimensions combined sorted from the best (Solution) and the worst configurations sorted from the worst (Dominated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paretoFronts_Q = multiDimensionRank.paretoQ()\n",
    "paretoFronts_Agg = multiDimensionRank.paretoAgg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the top 5 configurations according to paretoQ method sorted from best to worst\n",
    "paretoFronts_Q.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the top 5 configurations according to paretoAgg method sorted from best to worst\n",
    "paretoFronts_Agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>plot</b> method shows the solutions for _paretoAgg_ as shades of green areas projected in a three dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiDimensionRank.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Visualization</h2>\n",
    "*visualization can only be done with three dimension ranking configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Criteria Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This library provides two metrics of evaluation to evaluate the goodness of the ranking criteria the _conformance_ and _coherence_\n",
    "\n",
    "- Conformance measures the adherence of the top-ranked configurations according to the actual query positioning of thoses configurations. We calculate conformance according to the equation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$A(R^k) = 1 - \\sum \\limits _{i=0} ^{|Q|} \\sum \\limits _{j=0} ^{k} \\frac {\\bar{A}(i,j)}{|Q|*k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $R_{s}$ ranking and the top-3 ranked configurations are ${c_{1},c_{2},c_{3}}$, that overlaps only with the bottom-3 ranked configuration in query $|Q|$. That is, ${c_{4},c_{2},c_{5}}$. For example, $c_{2}$ is in the $59^{th}$ position out of 60 positions. <br>\n",
    "Thus, $A(R^k) = 1- \\frac {1}{(11*3)}$, when $k = 3$ and $|Q| = 11$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Coherence is the measure agreement between two ranking sets that uses the same ranking criteria accross different experiments. We used Kendall's Index to calculate coherence, which counts the number of dis(agreements) between two ranking sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K(R_{1}, R_{2}) = \\sum \\limits _{{i,j} \\epsilon P} ^{} \\frac {\\bar{K}_{i,j}(R_{1}, R_{2})}{|P|}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we assume that rank sets are the dataset sizes (i.e. 100M and 250M). Kendall’s distance between two rank sets $R_{1}$ and $R_{2}$, where $|P|$ represents the set of unique pairs of distinct elements in the two sets. For instance, the $K$ index between $R_{1}={c_{1},c_{2},c_{3}}$ and $R_{2}={c_{1},c_{2},c_{4}}$ for 100M and 250M is 0.33, i.e., one disagreement out of three pair comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conformance</b> class takes a ranking set for the user to specify along with the _k_ value and _h_ value for the conformance to calculate<br>\n",
    "- k_ value is the value of the top-k subset of the ranking sets\n",
    "- _h_ value is the threshold value that will be counted for the conformance score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Coherence</b> class counts the number of pairwise (dis)agreements between two ranking sets (i.e. dataset sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both conformance and coherence classes takes a list of ranking criterion that the user can specify\n",
    "from PAPyA.Ranker import Conformance, Coherence\n",
    "\n",
    "conformance_set = ['schemas', 'partition', 'storage', 'paretoQ', 'paretoAgg']\n",
    "coherence_set = ['schemas', 'partition', 'storage', 'paretoQ', 'paretoAgg']\n",
    "\n",
    "conf = Conformance(config, logs, '100M', conformance_set, 5, 28)\n",
    "coh = Coherence(config, logs,coherence_set, '100M', '250M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>run</b> method automates calculating the scores for _conformance_ (the higher the better) and _coherence_ (the lower the better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coh.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>showTable</b> method shows the table that we use when calculating the validation of the ranking criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _conformance_, we give an example for single dimension \"schemas\" ranking criterion. $k=5$ gets the top 5 configuration for this ranking criterion and $h=28$ takes the rank occurences between 11 queries that has the rank > 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conformance_set = ['schemas']\n",
    "conf = Conformance(config, logs, '100M', conformance_set, 6, 28)\n",
    "conf.showTable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For _coherence_, we give an example for single dimension \"partition\" ranking criterion with the ranking sets of 100M and 250M which will be counted for the number of pairwise (dis)agreements in the ranking sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_set = ['partition']\n",
    "coh = Coherence(config, logs,coherence_set, '100M', '250M')\n",
    "coh.showTable()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6ddfff14bf7760b9dea96af8633efca624fe761838f5b2d212d404c1943ff4ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
